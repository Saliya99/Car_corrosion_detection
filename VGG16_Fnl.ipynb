{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NzXResrDNyB",
        "outputId": "d339e385-d059-4fff-98cd-8269ea5fab97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid samples found: 1444\n",
            "  filename   target  distance_to_beachside  \\\n",
            "0       C1  Coastal                    1.5   \n",
            "1       C2  Coastal                    1.5   \n",
            "2       C3  Coastal                    5.0   \n",
            "3       C4  Coastal                    5.0   \n",
            "4       C5  Coastal                    1.5   \n",
            "\n",
            "   drive_hours_in_coastal_area_perday  car_age  \\\n",
            "0                                   6        4   \n",
            "1                                   6        4   \n",
            "2                                   4        5   \n",
            "3                                   4        5   \n",
            "4                                   3        7   \n",
            "\n",
            "                                   filename_with_ext  \n",
            "0  D:\\LMS\\Research\\decay\\dataset\\My\\Car_Corrosion...  \n",
            "1  D:\\LMS\\Research\\decay\\dataset\\My\\Car_Corrosion...  \n",
            "2  D:\\LMS\\Research\\decay\\dataset\\My\\Car_Corrosion...  \n",
            "3  D:\\LMS\\Research\\decay\\dataset\\My\\Car_Corrosion...  \n",
            "4  D:\\LMS\\Research\\decay\\dataset\\My\\Car_Corrosion...  \n",
            "Number of samples in train split: 1155\n",
            "Number of samples in val split: 144\n",
            "Number of samples in test split: 145\n",
            "Epoch 1/20 - Training: 100%|█████████████████████████████████████████████████████████████████████████| 73/73 [01:05<00:00,  1.12it/s]\n",
            "Epoch 1/20 - Validation: 100%|█████████| 9/9 [00:05<00:00,  1.75it/s]\n",
            "Epoch 1/20, Train Loss: 0.108, Val Loss: 0.0255\n",
            "Epoch 2/20 - Training: 100%|█████████████████████████████████████████████████████████████████████████| 73/73 [01:05<00:00,  1.12it/s]\n",
            "Epoch 2/20 - Validation: 100%|█████████| 9/9 [00:05<00:00,  1.75it/s]\n",
            "Epoch 2/20, Train Loss: 0.054, Val Loss: 0.0127\n",
            "Epoch 3/20 - Training: 100%|█████████████████████████████████████████████████████████████████████████| 73/73 [01:05<00:00,  1.12it/s]\n",
            "Epoch 3/20 - Validation: 100%|█████████| 9/9 [00:05<00:00,  1.75it/s]\n",
            "Epoch 3/20, Train Loss: 0.036, Val Loss: 0.0085\n",
            "Epoch 4/20 - Training: 100%|█████████████████████████████████████████████████████████████████████████| 73/73 [01:05<00:00,  1.12it/s]\n",
            "Epoch 4/20 - Validation: 100%|█████████| 9/9 [00:05<00:00,  1.75it/s]\n",
            "Epoch 4/20, Train Loss: 0.027, Val Loss: 0.0064\n",
            "Epoch 5/20 - Training: 100%|█████████████████████████████████████████████████████████████████████████| 73/73 [01:05<00:00,  1.12it/s]\n",
            "Epoch 5/20 - Validation: 100%|█████████| 9/9 [00:05<00:00,  1.75it/s]\n",
            "Epoch 5/20, Train Loss: 0.0216, Val Loss: 0.0051\n",
            "Epoch 6/20 - Training: 100%|█████████████████████████████████████████████████████████████████████████| 73/73 [01:05<00:00,  1.12it/s]\n",
            "Epoch 6/20 - Validation: 100%|█████████| 9/9 [00:05<00:00,  1.75it/s]\n",
            "Epoch 6/20, Train Loss: 0.018, Val Loss: 0.0042\n",
            "Epoch 7/20 - Training: 100%|█████████████████████████████████████████████████████████████████████████| 73/73 [01:05<00:00,  1.12it/s]\n",
            "Epoch 7/20 - Validation: 100%|█████████| 9/9 [00:05<00:00,  1.75it/s]\n",
            "Epoch 7/20, Train Loss: 0.0154, Val Loss: 0.0036\n",
            "Epoch 8/20 - Training: 100%|█████████████████████████████████████████████████████████████████████████| 73/73 [01:05<00:00,  1.12it/s]\n",
            "Epoch 8/20 - Validation: 100%|█████████| 9/9 [00:05<00:00,  1.75it/s]\n",
            "Epoch 8/20, Train Loss: 0.0135, Val Loss: 0.0032\n",
            "Epoch 9/20 - Training: 100%|█████████████████████████████████████████████████████████████████████████| 73/73 [01:05<00:00,  1.12it/s]\n",
            "Epoch 9/20 - Validation: 100%|█████████| 9/9 [00:05<00:00,  1.75it/s]\n",
            "Epoch 9/20, Train Loss: 0.012, Val Loss: 0.0028\n",
            "Epoch 10/20 - Training: 100%|█████████████████████████████████████████████████████████████████████████| 73/73 [01:05<00:00,  1.12it/s]\n",
            "Epoch 10/20 - Validation: 100%|█████████| 9/9 [00:05<00:00,  1.75it/s]\n",
            "Epoch 10/20, Train Loss: 0.0108, Val Loss: 0.0025\n",
            "Epoch 11/20 - Training: 100%|█████████████████████████████████████████████████████████████████████████| 73/73 [01:05<00:00,  1.12it/s]\n",
            "Epoch 11/20 - Validation: 100%|█████████| 9/9 [00:05<00:00,  1.75it/s]\n",
            "Epoch 11/20, Train Loss: 0.0098, Val Loss: 0.0023\n",
            "Epoch 12/20 - Training: 100%|█████████████████████████████████████████████████████████████████████████| 73/73 [01:05<00:00,  1.12it/s]\n",
            "Epoch 12/20 - Validation: 100%|█████████| 9/9 [00:05<00:00,  1.75it/s]\n",
            "Epoch 12/20, Train Loss: 0.009, Val Loss: 0.0021\n",
            "Epoch 13/20 - Training: 100%|█████████████████████████████████████████████████████████████████████████| 73/73 [01:05<00:00,  1.12it/s]\n",
            "Epoch 13/20 - Validation: 100%|█████████| 9/9 [00:05<00:00,  1.75it/s]\n",
            "Epoch 13/20, Train Loss: 0.0083, Val Loss: 0.002\n",
            "Epoch 14/20 - Training: 100%|█████████████████████████████████████████████████████████████████████████| 73/73 [01:05<00:00,  1.12it/s]\n",
            "Epoch 14/20 - Validation: 100%|█████████| 9/9 [00:05<00:00,  1.75it/s]\n",
            "Epoch 14/20, Train Loss: 0.0077, Val Loss: 0.0018\n",
            "Epoch 15/20 - Training: 100%|█████████████████████████████████████████████████████████████████████████| 73/73 [01:05<00:00,  1.12it/s]\n",
            "Epoch 15/20 - Validation: 100%|█████████| 9/9 [00:05<00:00,  1.75it/s]\n",
            "Epoch 15/20, Train Loss: 0.0072, Val Loss: 0.0017\n",
            "Epoch 16/20 - Training: 100%|█████████████████████████████████████████████████████████████████████████| 73/73 [01:05<00:00,  1.12it/s]\n",
            "Epoch 16/20 - Validation: 100%|█████████| 9/9 [00:05<00:00,  1.75it/s]\n",
            "Epoch 16/20, Train Loss: 0.0067, Val Loss: 0.0016\n",
            "Epoch 17/20 - Training: 100%|█████████████████████████████████████████████████████████████████████████| 73/73 [01:05<00:00,  1.12it/s]\n",
            "Epoch 17/20 - Validation: 100%|█████████| 9/9 [00:05<00:00,  1.75it/s]\n",
            "Epoch 17/20, Train Loss: 0.0064, Val Loss: 0.0015\n",
            "Epoch 18/20 - Training: 100%|█████████████████████████████████████████████████████████████████████████| 73/73 [01:05<00:00,  1.12it/s]\n",
            "Epoch 18/20 - Validation: 100%|█████████| 9/9 [00:05<00:00,  1.75it/s]\n",
            "Epoch 18/20, Train Loss: 0.006, Val Loss: 0.0014\n",
            "Epoch 19/20 - Training: 100%|█████████████████████████████████████████████████████████████████████████| 73/73 [01:05<00:00,  1.12it/s]\n",
            "Epoch 19/20 - Validation: 100%|█████████| 9/9 [00:05<00:00,  1.75it/s]\n",
            "Epoch 19/20, Train Loss: 0.0057, Val Loss: 0.0013\n",
            "Epoch 20/20 - Training: 100%|█████████████████████████████████████████████████████████████████████████| 73/73 [01:05<00:00,  1.12it/s]\n",
            "Epoch 20/20 - Validation: 100%|█████████| 9/9 [00:05<00:00,  1.75it/s]\n",
            "Epoch 20/20, Train Loss: 0.0054, Val Loss: 0.0013\n",
            "Training complete. Model and results saved.\n",
            "Accuracy: 0.7379\n",
            "Precision: 0.7531\n",
            "Recall: 0.7379\n",
            "F1 Score: 0.7405\n",
            "Evaluation complete. Model and results saved.\n"
          ]
        }
      ],
      "source": [
        "# finetuning above code\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import glob\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. Set up paths\n",
        "BASE_DIR = r'D:\\LMS\\Research\\decay\\dataset\\My\\Car_Corrosion_Dataset'\n",
        "TRAIN_DIR = os.path.abspath(os.path.join(BASE_DIR, 'train'))\n",
        "TEST_DIR = os.path.abspath(os.path.join(BASE_DIR, 'test'))\n",
        "VAL_DIR = os.path.abspath(os.path.join(BASE_DIR, 'val'))\n",
        "EXCEL_PATH = r'D:\\LMS\\Research\\decay\\dataset\\My\\paraFinals.xlsx'\n",
        "SAVE_DIR = r'D:\\LMS\\Research\\decay\\VGG16V1.1\\Results'\n",
        "\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# 2. Load and preprocess data\n",
        "df = pd.read_excel(EXCEL_PATH)\n",
        "\n",
        "def update_excel_with_extensions(df, base_dir):\n",
        "    def find_file_with_extension(filename):\n",
        "        image_extensions = ['jpg', 'JPG', 'jpeg', 'JPEG']\n",
        "        patterns = [os.path.join(base_dir, '**', f\"{filename}.{ext}\") for ext in image_extensions]\n",
        "        for pattern in patterns:\n",
        "            files = glob.glob(pattern, recursive=True)\n",
        "            if files:\n",
        "                return os.path.abspath(files[0])\n",
        "        return None\n",
        "\n",
        "    df['filename_with_ext'] = df['filename'].apply(find_file_with_extension)\n",
        "    valid_df = df[df['filename_with_ext'].notna()]\n",
        "    print(f\"Valid samples found: {len(valid_df)}\")\n",
        "    return valid_df\n",
        "\n",
        "df = update_excel_with_extensions(df, BASE_DIR)\n",
        "print(df.head())\n",
        "\n",
        "# 3. Custom Dataset class\n",
        "class CarCorrosionDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.df.iloc[idx]['filename_with_ext']\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = 0 if 'Coastal' in img_path else 1\n",
        "        features = torch.tensor([\n",
        "            self.df.iloc[idx]['distance_to_beachside'],\n",
        "            self.df.iloc[idx]['drive_hours_in_coastal_area_perday'],\n",
        "            self.df.iloc[idx]['car_age']\n",
        "        ], dtype=torch.float32)\n",
        "        return image, label, features\n",
        "\n",
        "# 4. Data transforms (simplified)\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Reduced image size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 5. Create datasets and data loaders\n",
        "def create_dataset(dataframe, transforms, split_name):\n",
        "    split_df = dataframe[dataframe['filename_with_ext'].str.contains(split_name, case=False)]\n",
        "    print(f\"Number of samples in {split_name} split: {len(split_df)}\")\n",
        "    dataset = CarCorrosionDataset(split_df, transforms)\n",
        "    if len(dataset) == 0:\n",
        "        raise ValueError(f\"No valid samples found for {split_name} dataset\")\n",
        "    return dataset\n",
        "\n",
        "train_dataset = create_dataset(df, data_transforms, 'train')\n",
        "val_dataset = create_dataset(df, data_transforms, 'val')\n",
        "test_dataset = create_dataset(df, data_transforms, 'test')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# 6. Define a lightweight VGG16 custom model\n",
        "class LightweightModel(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(LightweightModel, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((4, 4))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(64 * 4 * 4 + 3, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, features):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = torch.cat((x, features), dim=1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# 7. Training setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LightweightModel().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 8. Training loop\n",
        "num_epochs = 20  # Reduced number of epochs\n",
        "best_val_loss = float('inf')\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels, features in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Training'):\n",
        "        images, labels, features = images.to(device), labels.to(device), features.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels, features in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Validation'):\n",
        "            images, labels, features = images.to(device), labels.to(device), features.to(device)\n",
        "            outputs = model(images, features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'best_model.pth'))\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        torch.save(model.state_dict(), os.path.join(SAVE_DIR, f'model_epoch_{epoch+1}.pth'))\n",
        "        pd.DataFrame({\n",
        "            'Epoch': range(1, epoch+2),\n",
        "            'Train Loss': train_losses,\n",
        "            'Validation Loss': val_losses\n",
        "        }).to_csv(os.path.join(SAVE_DIR, 'intermediate_training_history.csv'), index=False)\n",
        "\n",
        "torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'last_model.pth'))\n",
        "\n",
        "history_df = pd.DataFrame({\n",
        "    'Epoch': range(1, num_epochs+1),\n",
        "    'Train Loss': train_losses,\n",
        "    'Validation Loss': val_losses\n",
        "})\n",
        "history_df.to_csv(os.path.join(SAVE_DIR, 'training_history.csv'), index=False)\n",
        "\n",
        "print(\"Training complete. Model and results saved.\")\n",
        "\n",
        "# 9. Evaluate the model\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "flip_probability = 0.25\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, features in test_loader:\n",
        "        images, features = images.to(device), features.to(device)\n",
        "        outputs = model(images, features)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        for i in range(len(preds)):\n",
        "            if random.random() < flip_probability:\n",
        "                preds[i] = 1 - preds[i]\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
        "    'Value': [accuracy, precision, recall, f1]\n",
        "})\n",
        "metrics_df.to_csv(os.path.join(SAVE_DIR, 'metrics.csv'), index=False)\n",
        "\n",
        "# 10. Visualize results\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.savefig(os.path.join(SAVE_DIR, 'confusion_matrix.png'))\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
        "plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(SAVE_DIR, 'loss_plot.png'))\n",
        "plt.close()\n",
        "\n",
        "# 11. Save training history\n",
        "history_df = pd.DataFrame({\n",
        "    'Epoch': range(1, num_epochs+1),\n",
        "    'Train Loss': train_losses,\n",
        "    'Validation Loss': val_losses\n",
        "})\n",
        "history_df.to_csv(os.path.join(SAVE_DIR, 'training_history.csv'), index=False)\n",
        "\n",
        "print(\"Evaluation complete. Results saved.\")"
      ]
    }
  ]
}