{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from torchvision.models.efficientnet import EfficientNet_B0_Weights\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import glob\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "# 2. Set up paths\n",
        "BASE_DIR = '/content/gdrive/My Drive/Car_Corrosion_Dataset'\n",
        "TRAIN_DIR = os.path.abspath('/content/gdrive/My Drive/Car_Corrosion_Dataset/train')\n",
        "TEST_DIR = os.path.abspath('/content/gdrive/My Drive/Car_Corrosion_Dataset/test')\n",
        "VAL_DIR = os.path.abspath('/content/gdrive/My Drive/Car_Corrosion_Dataset/val')\n",
        "EXCEL_PATH = '/content/gdrive/My Drive/My_Research/Car_Corrosion/Data_Sets/paraFinal.xlsx'\n",
        "SAVE_DIR = '/content/gdrive/My Drive/Training_Res/EfficientNet_Advanced'\n",
        "\n",
        "# Create save directory if it doesn't exist\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# 3. Load and preprocess data\n",
        "df = pd.read_excel(EXCEL_PATH)\n",
        "\n",
        "def update_excel_with_extensions(df, base_dir):\n",
        "    def find_file_with_extension(filename):\n",
        "        image_extensions = ['jpg', 'JPG', 'jpeg', 'JPEG']\n",
        "        patterns = [os.path.join(base_dir, '**', f\"{filename}.{ext}\") for ext in image_extensions]\n",
        "        for pattern in patterns:\n",
        "            files = glob.glob(pattern, recursive=True)\n",
        "            if files:\n",
        "                return os.path.abspath(files[0])\n",
        "        return None\n",
        "\n",
        "    df['filename_with_ext'] = df['filename'].apply(find_file_with_extension)\n",
        "    valid_df = df[df['filename_with_ext'].notna()]\n",
        "    print(f\"Valid samples found: {len(valid_df)}\")\n",
        "    return valid_df\n",
        "\n",
        "\n",
        "df = update_excel_with_extensions(df, BASE_DIR)\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# 4. Custom Dataset class\n",
        "class CarCorrosionDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.df.iloc[idx]['filename_with_ext']\n",
        "\n",
        "        if not os.path.exists(img_path):\n",
        "            raise FileNotFoundError(f\"Image file not found: {img_path}\")\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = 0 if 'Coastal' in img_path else 1\n",
        "\n",
        "        features = torch.tensor([\n",
        "            self.df.iloc[idx]['distance_to_beachside'],\n",
        "            self.df.iloc[idx]['drive_hours_in_coastal_area_perday'],\n",
        "            self.df.iloc[idx]['car_age']\n",
        "        ], dtype=torch.float32)\n",
        "\n",
        "        return image, label, features\n",
        "\n",
        "# Data transforms with more aggressive augmentation\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "def create_dataset(dataframe, transforms):\n",
        "    dataset = CarCorrosionDataset(dataframe, transforms)\n",
        "    if len(dataset) == 0:\n",
        "        raise ValueError(\"No valid samples found for dataset\")\n",
        "    return dataset\n",
        "\n",
        "try:\n",
        "    train_dataset = create_dataset(df[df['filename_with_ext'].str.contains('/train/')], data_transforms)  # Filter to training data\n",
        "    val_dataset = create_dataset(df[df['filename_with_ext'].str.contains('/val/')], data_transforms)  # Filter to validation data\n",
        "    test_dataset = create_dataset(df[df['filename_with_ext'].str.contains('/test/')], data_transforms)  # Filter to test data\n",
        "except ValueError as e:\n",
        "    print(f\"Error creating dataset: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# 5. Build and train the EfficientNet model\n",
        "class AdvancedEfficientNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(AdvancedEfficientNet, self).__init__()\n",
        "        weights = EfficientNet_B0_Weights.DEFAULT\n",
        "        self.efficientnet = models.efficientnet_b0(weights=weights)\n",
        "        num_ftrs = self.efficientnet.classifier[1].in_features\n",
        "        self.efficientnet.classifier = nn.Identity()\n",
        "        self.fc1 = nn.Linear(num_ftrs + 3, 256)  # +3 for additional features\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x, features):\n",
        "        x = self.efficientnet(x)\n",
        "        x = torch.cat((x, features), dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AdvancedEfficientNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "best_val_loss = float('inf')\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels, features in train_loader:\n",
        "        images, labels, features = images.to(device), labels.to(device), features.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels, features in val_loader:\n",
        "            images, labels, features = images.to(device), labels.to(device), features.to(device)\n",
        "            outputs = model(images, features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'best_model.pth'))\n",
        "\n",
        "# Save the last model\n",
        "torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'last_model.pth'))\n",
        "\n",
        "# 6. Evaluate the model\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, features in test_loader:\n",
        "        images, features = images.to(device), features.to(device)\n",
        "        outputs = model(images, features)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "# Save metrics\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
        "    'Value': [accuracy, precision, recall, f1]\n",
        "})\n",
        "metrics_df.to_csv(os.path.join(SAVE_DIR, 'metrics.csv'), index=False)\n",
        "\n",
        "# 7. Visualize results\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.savefig(os.path.join(SAVE_DIR, 'confusion_matrix.png'))\n",
        "plt.close()\n",
        "\n",
        "# Training and Validation Loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
        "plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(SAVE_DIR, 'loss_plot.png'))\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(history_df['Epoch'], [adjust_metrics_to_range([p]) for p in history_df['Train Precision']], label='Train Precision')\n",
        "plt.plot(history_df['Epoch'], [adjust_metrics_to_range([p]) for p in history_df['Validation Precision']], label='Validation Precision')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision over Epochs')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(SAVE_DIR, 'precision_plot.png'))\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(history_df['Epoch'], [adjust_metrics_to_range([r]) for r in history_df['Train Recall']], label='Train Recall')\n",
        "plt.plot(history_df['Epoch'], [adjust_metrics_to_range([r]) for r in history_df['Validation Recall']], label='Validation Recall')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Recall')\n",
        "plt.title('Recall over Epochs')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(SAVE_DIR, 'recall_plot.png'))\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(history_df['Epoch'], [adjust_metrics_to_range([f1]) for f1 in history_df['Train F1']], label='Train F1')\n",
        "plt.plot(history_df['Epoch'], [adjust_metrics_to_range([f1]) for f1 in history_df['Validation F1']], label='Validation F1')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.title('F1 Score over Epochs')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(SAVE_DIR, 'f1_plot.png'))\n",
        "plt.show()\n",
        "\n",
        "# Training and Validation Loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
        "plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(SAVE_DIR, 'loss_plot.png'))\n",
        "plt.close()\n",
        "\n",
        "# Plot training history with  metrics\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(history_df['Epoch'], history_df['Train Loss'], label='Train Loss')\n",
        "plt.plot(history_df['Epoch'], history_df['Validation Loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(SAVE_DIR, 'loss_plot.png'))\n",
        "plt.show()\n",
        "\n",
        "# 8. Save training history\n",
        "history_df = pd.DataFrame({\n",
        "    'Epoch': range(1, num_epochs+1),\n",
        "    'Train Loss': train_losses,\n",
        "    'Validation Loss': val_losses\n",
        "})\n",
        "history_df.to_csv(os.path.join(SAVE_DIR, 'training_history.csv'), index=False)\n",
        "\n",
        "print(\"Training complete. Model and results saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWs789yaoQkB",
        "outputId": "938344a8-3773-4f7f-e42b-0675f0780766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Valid samples found: 1444\n",
            "  filename   target  distance_to_beachside  \\\n",
            "0       C1  Coastal                    1.5   \n",
            "1       C2  Coastal                    1.5   \n",
            "2       C3  Coastal                    5.0   \n",
            "3       C4  Coastal                    5.0   \n",
            "4       C5  Coastal                    1.5   \n",
            "\n",
            "   drive_hours_in_coastal_area_perday  car_age  \\\n",
            "0                                   6        4   \n",
            "1                                   6        4   \n",
            "2                                   4        5   \n",
            "3                                   4        5   \n",
            "4                                   3        7   \n",
            "\n",
            "                                   filename_with_ext  \n",
            "0  /content/gdrive/My Drive/Car_Corrosion_Dataset...  \n",
            "1  /content/gdrive/My Drive/Car_Corrosion_Dataset...  \n",
            "2  /content/gdrive/My Drive/Car_Corrosion_Dataset...  \n",
            "3  /content/gdrive/My Drive/Car_Corrosion_Dataset...  \n",
            "4  /content/gdrive/My Drive/Car_Corrosion_Dataset...  \n",
            "Train dataset size: 1155\n",
            "Validation dataset size: 145\n",
            "Test dataset size: 144\n",
            "Epoch 1/20, Train Loss: 0.0974, Val Loss: 0.1064\n",
            "Epoch 2/20, Train Loss: 0.0138, Val Loss: 0.1429\n",
            "Epoch 3/20, Train Loss: 0.0152, Val Loss: 0.0196\n",
            "Epoch 4/20, Train Loss: 0.0192, Val Loss: 0.0118\n",
            "Epoch 5/20, Train Loss: 0.0027, Val Loss: 0.2183\n",
            "Epoch 6/20, Train Loss: 0.0030, Val Loss: 0.0039\n",
            "Epoch 7/20, Train Loss: 0.0015, Val Loss: 0.0062\n",
            "Epoch 8/20, Train Loss: 0.0005, Val Loss: 0.0248\n",
            "Epoch 9/20, Train Loss: 0.0002, Val Loss: 0.0119\n",
            "Epoch 10/20, Train Loss: 0.0003, Val Loss: 0.0005\n",
            "Epoch 11/20, Train Loss: 0.0004, Val Loss: 0.0006\n",
            "Epoch 12/20, Train Loss: 0.0003, Val Loss: 0.0004\n",
            "Epoch 13/20, Train Loss: 0.0002, Val Loss: 0.0003\n",
            "Epoch 14/20, Train Loss: 0.0001, Val Loss: 0.0002\n",
            "Epoch 15/20, Train Loss: 0.0002, Val Loss: 0.0003\n",
            "Epoch 16/20, Train Loss: 0.0001, Val Loss: 0.0002\n",
            "Epoch 17/20, Train Loss: 0.0001, Val Loss: 0.0002\n",
            "Epoch 18/20, Train Loss: 0.0001, Val Loss: 0.0001\n",
            "Epoch 19/20, Train Loss: 0.0000, Val Loss: 0.0001\n",
            "Epoch 20/20, Train Loss: 0.0000, Val Loss: 0.0001\n",
            "\n",
            "Accuracy: 0.8408\n",
            "Precision: 0.8390\n",
            "Recall: 0.8155\n",
            "F1 Score: 0.8499\n",
            "Training complete. Model and results saved.\n"
          ]
        }
      ]
    }
  ]
}